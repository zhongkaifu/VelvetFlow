"""Structure planning logic for the planner."""

import json
import os
from typing import Any, Dict, List, Mapping, Optional

from openai import OpenAI

from velvetflow.config import OPENAI_MODEL
from velvetflow.logging_utils import (
    child_span,
    log_debug,
    log_error,
    log_event,
    log_info,
    log_json,
    log_llm_usage,
    log_section,
    log_success,
    log_warn,
)
from velvetflow.planner.action_guard import ensure_registered_actions
from velvetflow.planner.approval import detect_missing_approval_nodes
from velvetflow.planner.connectivity import ensure_edges_connectivity
from velvetflow.planner.coverage import (
    check_requirement_coverage_with_llm,
    refine_workflow_structure_with_llm,
)
from velvetflow.planner.llm_edges import synthesize_edges_with_llm
from velvetflow.planner.tools import PLANNER_TOOLS
from velvetflow.planner.workflow_builder import WorkflowBuilder
from velvetflow.search import HybridActionSearchService


def _build_action_schema_map(action_registry: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
    action_schemas: Dict[str, Dict[str, Any]] = {}
    for action in action_registry:
        aid = action.get("action_id")
        if not aid:
            continue
        action_schemas[aid] = {
            "name": action.get("name", ""),
            "description": action.get("description", ""),
            "domain": action.get("domain", ""),
            "arg_schema": action.get("arg_schema"),
            "output_schema": action.get("output_schema"),
        }
    return action_schemas


def _extract_loop_body_context(
    loop_node: Mapping[str, Any], action_schemas: Mapping[str, Mapping[str, Any]]
) -> Dict[str, Any]:
    params = loop_node.get("params") if isinstance(loop_node, Mapping) else None
    body = params.get("body_subgraph") if isinstance(params, Mapping) else None
    if not isinstance(body, Mapping):
        return {"nodes": [], "entry": None, "exit": None}

    context_nodes = []
    for child in body.get("nodes", []) or []:
        if not isinstance(child, Mapping):
            continue
        action_id = child.get("action_id")
        schema = action_schemas.get(action_id, {}) if isinstance(action_id, str) else {}
        context_nodes.append(
            {
                "id": child.get("id"),
                "type": child.get("type"),
                "action_id": action_id,
                "display_name": child.get("display_name"),
                "output_schema": schema.get("output_schema"),
            }
        )

    return {
        "nodes": context_nodes,
        "entry": body.get("entry"),
        "exit": body.get("exit"),
    }


def _fallback_loop_exports(
    loop_node: Mapping[str, Any], action_schemas: Mapping[str, Mapping[str, Any]]
) -> Optional[Dict[str, Any]]:
    params = loop_node.get("params") if isinstance(loop_node, Mapping) else None
    if not isinstance(params, Mapping):
        return None
    body = params.get("body_subgraph")
    if not isinstance(body, Mapping):
        return None

    body_nodes = [bn for bn in body.get("nodes", []) or [] if isinstance(bn, Mapping)]
    body_ids = [bn.get("id") for bn in body_nodes if isinstance(bn.get("id"), str)]
    exit_node = body.get("exit") if isinstance(body.get("exit"), str) else None
    from_node = exit_node if exit_node in body_ids else (body_ids[0] if body_ids else None)
    if not from_node:
        return None

    field_candidates: List[str] = []
    target_node = next((bn for bn in body_nodes if bn.get("id") == from_node), None)
    if isinstance(target_node, Mapping):
        action_id = target_node.get("action_id")
        schema = action_schemas.get(action_id, {}) if isinstance(action_id, str) else {}
        props = schema.get("output_schema", {}).get("properties") if isinstance(schema.get("output_schema"), Mapping) else None
        if isinstance(props, Mapping):
            field_candidates = [k for k in props.keys() if isinstance(k, str)]

    fields = field_candidates[:4] if field_candidates else ["status"]
    return {
        "items": {
            "from_node": from_node,
            "fields": fields,
            "mode": "collect",
        },
        "aggregates": [],
    }


def _ensure_loop_items_fields(
    *,
    exports: Mapping[str, Any],
    loop_node: Mapping[str, Any],
    action_schemas: Mapping[str, Mapping[str, Any]],
) -> Dict[str, Any]:
    """Ensure items.fields is a non-empty list.

    If the original exports already contains non-empty fields, it will be
    returned unchanged. Otherwise, we try to infer several representative
    fields from the referenced body node's output schema; fall back to a
    single "status" field if nothing is available.
    """

    items_spec = exports.get("items")
    if not isinstance(items_spec, Mapping):
        return dict(exports)

    fields = items_spec.get("fields") if isinstance(items_spec.get("fields"), list) else []
    normalized_fields = [f for f in fields if isinstance(f, str)]
    if normalized_fields:
        return exports

    params = loop_node.get("params") if isinstance(loop_node.get("params"), Mapping) else {}
    body = params.get("body_subgraph") if isinstance(params, Mapping) else {}
    body_nodes = [bn for bn in body.get("nodes", []) or [] if isinstance(bn, Mapping)]
    target_id = items_spec.get("from_node") if isinstance(items_spec.get("from_node"), str) else None
    target_node = next((bn for bn in body_nodes if bn.get("id") == target_id), None)

    fallback_fields: list[str] = []
    if isinstance(target_node, Mapping):
        action_id = target_node.get("action_id") if isinstance(target_node.get("action_id"), str) else None
        schema = action_schemas.get(action_id, {}) if isinstance(action_id, str) else {}
        props = schema.get("output_schema", {}).get("properties") if isinstance(schema.get("output_schema"), Mapping) else None
        if isinstance(props, Mapping):
            fallback_fields = [k for k in props.keys() if isinstance(k, str)]

    if not fallback_fields:
        fallback_fields = ["status"]

    new_items = dict(items_spec)
    new_items["fields"] = fallback_fields[:4]
    new_exports = dict(exports)
    new_exports["items"] = new_items
    return new_exports


def _synthesize_loop_exports_with_llm(
    *,
    client: OpenAI,
    model: str,
    nl_requirement: str,
    loop_node: Mapping[str, Any],
    action_schemas: Mapping[str, Mapping[str, Any]],
) -> Optional[Dict[str, Any]]:
    body_context = _extract_loop_body_context(loop_node, action_schemas)
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“é—¨ä¸ºå¾ªç¯èŠ‚ç‚¹è®¾è®¡ exports çš„åŠ©æ‰‹ã€‚\n"
        "ç»™å®š loop èŠ‚ç‚¹ï¼ˆå« body_subgraphï¼‰ä»¥åŠä¸Šæ¸¸çš„è‡ªç„¶è¯­è¨€éœ€æ±‚ï¼Œ"
        "è¯·è¾“å‡ºç¬¦åˆ DSL çš„ exports ç»“æ„ï¼Œç”¨äºå°†å¾ªç¯å­å›¾çš„ç»“æœæš´éœ²ç»™å¤–éƒ¨èŠ‚ç‚¹ã€‚\n"
        "è¦æ±‚ï¼š\n"
        "1) åªè¾“å‡º JSONï¼ˆä¸è¦ä»£ç å—ï¼‰ï¼Œæ ¼å¼å¯ä»¥æ˜¯ {\"exports\": {...}} æˆ–ç›´æ¥ exports å¯¹è±¡ã€‚\n"
        "2) items.from_node å¿…é¡»å¼•ç”¨ body_subgraph.nodes ä¸­çš„èŠ‚ç‚¹ï¼ˆé€šå¸¸æ˜¯ exit èŠ‚ç‚¹ï¼‰ï¼Œfields éœ€åˆ—å‡ºä½ å¸Œæœ›æš´éœ²çš„å­—æ®µã€‚\n"
        "3) aggregates æ˜¯å¯é€‰çš„ count_if/max/min/sum/avg èšåˆï¼Œfrom_node åŒæ ·åªèƒ½æŒ‡å‘ body_subgraph èŠ‚ç‚¹ã€‚\n"
        "4) é¿å…è‡ªç„¶è¯­è¨€è§£é‡Šï¼Œä½¿ç”¨ç»“æ„åŒ–è¡¨è¾¾å¼ï¼Œå­—æ®µåä¼˜å…ˆä¾æ®èŠ‚ç‚¹ output_schema.propertiesã€‚\n"
        "ç¤ºä¾‹ï¼ˆä»…ç¤ºæ„ï¼Œä¸è¦ç”Ÿæ¬ç¡¬å¥—å­—æ®µåï¼‰ï¼š\n"
        "{\n  \"items\": {\"from_node\": \"finish_employee\", \"fields\": [\"employee_id\", \"risk\"], \"mode\": \"collect\"},\n"
        " \"aggregates\": [{\"name\": \"high_risk_count\", \"from_node\": \"finish_employee\", \"expr\": {\"kind\": \"count_if\", \"field\": \"risk\", \"op\": \">\", \"value\": 0.8}}]\n}"
    )

    payload = {
        "nl_requirement": nl_requirement,
        "loop_node": loop_node,
        "loop_body": body_context,
        "hint": "ä¼˜å…ˆé€‰æ‹© body_subgraph.exit ä½œä¸º items.from_nodeï¼Œå­—æ®µæ¥è‡ªè¯¥èŠ‚ç‚¹ output_schema.propertiesã€‚",
    }

    with child_span("loop_exports_llm"):
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": json.dumps(payload, ensure_ascii=False)},
            ],
            temperature=0.2,
        )
    log_llm_usage(model, getattr(resp, "usage", None), operation="synthesize_loop_exports")

    content = resp.choices[0].message.content or ""
    text = content.strip()
    if text.startswith("```"):
        text = text.strip("`")
        if "\n" in text:
            first_line, rest = text.split("\n", 1)
            if first_line.strip().lower().startswith("json"):
                text = rest

    decoder = json.JSONDecoder()
    parsed: Any
    try:
        parsed, _ = decoder.raw_decode(text)
    except json.JSONDecodeError:
        return None

    if isinstance(parsed, Mapping):
        exports = parsed.get("exports") if "exports" in parsed else parsed
        if isinstance(exports, Mapping):
            return dict(exports)
    return None


def _ensure_loop_exports_with_llm(
    *,
    workflow: Dict[str, Any],
    action_registry: List[Dict[str, Any]],
    nl_requirement: str,
    model: str,
) -> Dict[str, Any]:
    nodes = workflow.get("nodes", []) if isinstance(workflow, Mapping) else []
    loop_nodes = [n for n in nodes if isinstance(n, Mapping) and n.get("type") == "loop"]
    if not loop_nodes:
        return workflow

    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    action_schemas = _build_action_schema_map(action_registry)

    new_nodes: List[Dict[str, Any]] = []
    for node in nodes:
        if not isinstance(node, Mapping) or node.get("type") != "loop":
            new_nodes.append(node)
            continue

        params = node.get("params") if isinstance(node.get("params"), Mapping) else {}
        exports = params.get("exports") if isinstance(params, Mapping) else None
        if isinstance(exports, Mapping) and exports:
            ensured_exports = _ensure_loop_items_fields(
                exports=exports, loop_node=node, action_schemas=action_schemas
            )
            new_params = dict(params)
            new_params["exports"] = ensured_exports
            new_node = dict(node)
            new_node["params"] = new_params
            new_nodes.append(new_node)
            continue

        synthesized = _synthesize_loop_exports_with_llm(
            client=client,
            model=model,
            nl_requirement=nl_requirement,
            loop_node=node,
            action_schemas=action_schemas,
        )
        if not synthesized:
            synthesized = _fallback_loop_exports(node, action_schemas) or {}
            log_warn(
                f"[Planner] LLM æœªèƒ½ç”Ÿæˆ exportsï¼Œloop èŠ‚ç‚¹ {node.get('id')} ä½¿ç”¨å…œåº• exportsã€‚"
            )
        else:
            log_info(f"[Planner] LLM å·²ä¸º loop èŠ‚ç‚¹ {node.get('id')} ç”Ÿæˆ exportsã€‚")

        ensured_exports = _ensure_loop_items_fields(
            exports=synthesized, loop_node=node, action_schemas=action_schemas
        )
        new_params = dict(params)
        new_params["exports"] = ensured_exports
        new_node = dict(node)
        new_node["params"] = new_params
        new_nodes.append(new_node)

    new_workflow = dict(workflow)
    new_workflow["nodes"] = new_nodes
    return new_workflow


def plan_workflow_structure_with_llm(
    nl_requirement: str,
    search_service: HybridActionSearchService,
    action_registry: List[Dict[str, Any]],
    max_rounds: int = 10,
    max_coverage_refine_rounds: int = 2,
) -> Dict[str, Any]:
    client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
    builder = WorkflowBuilder()
    last_action_candidates: List[str] = []

    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªé€šç”¨ä¸šåŠ¡å·¥ä½œæµç¼–æ’åŠ©æ‰‹ã€‚\n"
        "ç³»ç»Ÿä¸­æœ‰ä¸€ä¸ª Action Registryï¼ŒåŒ…å«å¤§é‡ä¸šåŠ¡åŠ¨ä½œï¼Œä½ åªèƒ½é€šè¿‡ search_business_actions æŸ¥è¯¢ã€‚\n"
        "æ„å»ºæ–¹å¼ï¼š\n"
        "1) ä½¿ç”¨ set_workflow_meta è®¾ç½®å·¥ä½œæµåç§°å’Œæè¿°ã€‚\n"
        "2) å½“éœ€è¦ä¸šåŠ¡åŠ¨ä½œæ—¶ï¼Œå¿…é¡»å…ˆç”¨ search_business_actions æŸ¥è¯¢å€™é€‰ï¼›add_node(type='action') çš„ action_id å¿…é¡»å–è‡ªæœ€è¿‘ä¸€æ¬¡ candidates.idã€‚\n"
        "3) ä½¿ç”¨ add_edge è¿æ¥èŠ‚ç‚¹å½¢æˆæœ‰å‘å›¾ï¼ˆDAGï¼‰ï¼ŒåŒ…å«å¿…è¦çš„æ¡ä»¶/åˆ†æ”¯/å¾ªç¯/å¹¶è¡Œç­‰ã€‚\n"
        "4) å½“ç»“æ„å®Œæˆæ—¶è°ƒç”¨ finalize_workflowã€‚\n\n"
        "ã€éå¸¸é‡è¦çš„åŸåˆ™ã€‘\n"
        "1. æ‰€æœ‰ç¤ºä¾‹ï¼ˆåŒ…æ‹¬åç»­ä½ åœ¨è¡¥å‚é˜¶æ®µçœ‹åˆ°çš„ç¤ºä¾‹ï¼‰éƒ½åªæ˜¯ä¸ºè¯´æ˜â€œDSL çš„å†™æ³•â€å’Œâ€œèŠ‚ç‚¹ä¹‹é—´å¦‚ä½•è¿çº¿â€ï¼Œ\n"
        "   ä¸æ˜¯å®é™…çš„ä¸šåŠ¡çº¦æŸï¼Œä¸è¦åœ¨æ–°ä»»åŠ¡é‡Œç¡¬å¤ç”¨è¿™äº›ç¤ºä¾‹ä¸­çš„ä¸šåŠ¡åæˆ–å­—æ®µåã€‚\n"
        "2. ä½ å¿…é¡»ä¸¥æ ¼å›´ç»•å½“å‰å¯¹è¯ä¸­çš„è‡ªç„¶è¯­è¨€éœ€æ±‚æ¥è®¾è®¡ workflowï¼š\n"
        "   - è§¦å‘æ–¹å¼ï¼ˆå®šæ—¶ / äº‹ä»¶ / æ‰‹åŠ¨ï¼‰\n"
        "   - æ•°æ®æŸ¥è¯¢/è¯»å–\n"
        "   - ç­›é€‰/è¿‡æ»¤æ¡ä»¶\n"
        "   - èšåˆ/ç»Ÿè®¡/æ€»ç»“\n"
        "   - é€šçŸ¥ / å†™å…¥ / è½åº“ / è°ƒç”¨ä¸‹æ¸¸ç³»ç»Ÿ\n"
        "3. ä¸å…è®¸ä¸ºäº†æ¨¡ä»¿ç¤ºä¾‹ï¼Œè€Œåœ¨ä¸å½“å‰ä»»åŠ¡æ— å…³çš„æƒ…å†µä¸‹å¼•å…¥â€œå¥åº·/ä½“æ¸©/æ–°é—»/Nvidia/å‘˜å·¥/HRâ€ç­‰å…·ä½“è¯æ±‡ã€‚\n\n"
        "4. å¾ªç¯èŠ‚ç‚¹çš„å†…éƒ¨æ•°æ®åªèƒ½é€šè¿‡ loop.exports æš´éœ²ç»™å¤–éƒ¨ï¼Œä¸‹æ¸¸å¼•ç”¨å¾ªç¯ç»“æœæ—¶å¿…é¡»ä½¿ç”¨ result_of.<loop_id>.items æˆ– result_of.<loop_id>.aggregates.*ï¼Œç¦æ­¢ç›´æ¥å¼•ç”¨ body å­å›¾çš„èŠ‚ç‚¹ã€‚\n\n"
        "ã€è¦†ç›–åº¦è¦æ±‚ã€‘\n"
        "ä½ å¿…é¡»ç¡®ä¿å·¥ä½œæµç»“æ„èƒ½å¤Ÿå®Œå…¨è¦†ç›–ç”¨æˆ·è‡ªç„¶è¯­è¨€éœ€æ±‚ä¸­çš„æ¯ä¸ªå­ä»»åŠ¡ï¼Œè€Œä¸æ˜¯åªè¦†ç›–å‰åŠéƒ¨åˆ†ï¼š\n"
        "ä¾‹å¦‚ï¼Œå¦‚æœéœ€æ±‚åŒ…å«ï¼šè§¦å‘ + æŸ¥è¯¢ + ç­›é€‰ + æ€»ç»“ + é€šçŸ¥ï¼Œä½ ä¸èƒ½åªå®ç°è§¦å‘ + æŸ¥è¯¢ï¼Œ\n"
        "å¿…é¡»åœ¨ç»“æ„é‡Œæ˜¾å¼åŒ…å«ç­›é€‰ã€æ€»ç»“ã€é€šçŸ¥ç­‰å¯¹åº”èŠ‚ç‚¹å’Œæ•°æ®æµã€‚\n"
        "å½“ä½ ç¡®ä¿¡æ‰€æœ‰å­éœ€æ±‚éƒ½æœ‰å¯¹åº”çš„èŠ‚ç‚¹å’Œè¾¹æ—¶ï¼Œå†è°ƒç”¨ finalize_workflowã€‚"
    )

    messages: List[Dict[str, Any]] = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": nl_requirement},
    ]

    finalized = False

    # ---------- ç»“æ„è§„åˆ’ï¼ˆå¤šè½® tool-callingï¼‰ ----------
    for round_idx in range(max_rounds):
        log_section(f"ç»“æ„è§„åˆ’ Round {round_idx + 1}")
        with child_span("structure_planning_llm"):
            resp = client.chat.completions.create(
                model=OPENAI_MODEL,
                messages=messages,
                tools=PLANNER_TOOLS,
                tool_choice="auto",
                temperature=0.2,
            )
        log_llm_usage(OPENAI_MODEL, getattr(resp, "usage", None), operation="structure_planning")
        msg = resp.choices[0].message
        messages.append({
            "role": "assistant",
            "content": msg.content or "",
            "tool_calls": msg.tool_calls,
        })

        if not msg.tool_calls:
            log_warn("[Planner] æœ¬è½®æ²¡æœ‰ tool_callsï¼Œæå‰ç»“æŸã€‚")
            break

        for tc in msg.tool_calls:
            func_name = tc.function.name
            raw_args = tc.function.arguments
            tool_call_id = tc.id
            try:
                args = json.loads(raw_args) if raw_args else {}
            except json.JSONDecodeError:
                log_error(f"[Error] è§£æå·¥å…·å‚æ•°å¤±è´¥: {raw_args}")
                args = {}

            log_info(f"[Planner] è°ƒç”¨å·¥å…·: {func_name}({args})")

            if func_name == "search_business_actions":
                query = args.get("query", "")
                top_k = int(args.get("top_k", 5))
                actions_raw = search_service.search(query=query, top_k=top_k)
                candidates = [
                    {
                        "id": a.get("action_id"),
                        "name": a.get("name", ""),
                        "description": a.get("description", ""),
                        "category": a.get("domain") or "general",
                    }
                    for a in actions_raw
                    if a.get("action_id")
                ]
                last_action_candidates = [c["id"] for c in candidates]
                tool_result = {
                    "status": "ok",
                    "query": query,
                    "actions": actions_raw,
                    "candidates": candidates,
                }

            elif func_name == "set_workflow_meta":
                builder.set_meta(args.get("workflow_name", ""), args.get("description"))
                tool_result = {"status": "ok", "type": "meta_set"}

            elif func_name == "add_node":
                node_type = args["type"]
                action_id = args.get("action_id")

                if node_type == "action":
                    if not last_action_candidates:
                        tool_result = {
                            "status": "error",
                            "message": "action èŠ‚ç‚¹å¿…é¡»åœ¨è°ƒç”¨ search_business_actions ä¹‹ååˆ›å»ºï¼Œè¯·å…ˆæŸ¥è¯¢å€™é€‰åŠ¨ä½œã€‚",
                        }
                    elif action_id not in last_action_candidates:
                        tool_result = {
                            "status": "error",
                            "message": "action_id å¿…é¡»æ˜¯æœ€è¿‘ä¸€æ¬¡ search_business_actions è¿”å›çš„ candidates.id ä¹‹ä¸€ã€‚",
                            "allowed_action_ids": last_action_candidates,
                        }
                    else:
                        builder.add_node(
                            node_id=args["id"],
                            node_type=node_type,
                            action_id=action_id,
                            display_name=args.get("display_name"),
                            params=args.get("params") or {},
                        )
                        tool_result = {"status": "ok", "type": "node_added", "node_id": args["id"]}
                else:
                    builder.add_node(
                        node_id=args["id"],
                        node_type=node_type,
                        action_id=action_id,
                        display_name=args.get("display_name"),
                        params=args.get("params") or {},
                    )
                    tool_result = {"status": "ok", "type": "node_added", "node_id": args["id"]}

            elif func_name == "add_edge":
                builder.add_edge(
                    from_node=args["from_node"],
                    to_node=args["to_node"],
                    condition=args.get("condition"),
                )
                tool_result = {"status": "ok", "type": "edge_added"}

            elif func_name == "finalize_workflow":
                finalized = True
                tool_result = {"status": "ok", "type": "finalized", "notes": args.get("notes")}

            else:
                tool_result = {"status": "error", "message": f"æœªçŸ¥å·¥å…· {func_name}"}

            messages.append({
                "role": "tool",
                "tool_call_id": tool_call_id,
                "content": json.dumps(tool_result, ensure_ascii=False),
            })

        if finalized:
            log_success("[Planner] æ”¶åˆ° finalize_workflowï¼Œç»“æŸç»“æ„è§„åˆ’ã€‚")
            break

    # ---------- æ¥çº¿ + è¿é€šæ€§è¡¥å…¨ ----------
    skeleton = builder.to_workflow()
    nodes = skeleton.get("nodes", [])
    edges = skeleton.get("edges", [])

    if not edges:
        log_warn("[Planner] ç¬¬ä¸€é˜¶æ®µæ²¡æœ‰ç”Ÿæˆä»»ä½• edgesï¼Œè°ƒç”¨ LLM è¿›è¡Œè‡ªåŠ¨æ¥çº¿...")
        auto_edges = synthesize_edges_with_llm(nodes=nodes, nl_requirement=nl_requirement)
        if auto_edges:
            log_info(f"[Planner] LLM è‡ªåŠ¨ç”Ÿæˆäº† {len(auto_edges)} æ¡ edgesã€‚")
            skeleton["edges"] = auto_edges
        else:
            log_warn("[Planner] LLM è‡ªåŠ¨æ¥çº¿å¤±è´¥ï¼Œä½¿ç”¨ä¿åº•çº¿æ€§ä¸²è”æ–¹å¼ç”Ÿæˆ edgesã€‚")
            start_nodes = [n for n in nodes if n.get("type") == "start"]
            end_nodes = [n for n in nodes if n.get("type") == "end"]
            middle_nodes = [n for n in nodes if n.get("type") not in ("start", "end")]

            ordered = start_nodes + middle_nodes + end_nodes
            auto_edges = []
            for i in range(len(ordered) - 1):
                auto_edges.append({
                    "from": ordered[i]["id"],
                    "to": ordered[i + 1]["id"],
                    "condition": None,
                })
            skeleton["edges"] = auto_edges

    skeleton["edges"] = ensure_edges_connectivity(nodes, skeleton["edges"])
    skeleton = ensure_registered_actions(
        skeleton, action_registry=action_registry, search_service=search_service
    )

    # ---------- è¦†ç›–åº¦æ ¡éªŒ + ç»“æ„æ”¹è¿› ----------
    for refine_round in range(max_coverage_refine_rounds + 1):
        log_section(f"è¦†ç›–åº¦æ ¡éªŒè½®æ¬¡ {refine_round}")
        coverage = check_requirement_coverage_with_llm(
            nl_requirement=nl_requirement,
            workflow=skeleton,
            model=OPENAI_MODEL,
        )
        approval_missing = detect_missing_approval_nodes(
            workflow=skeleton, action_registry=action_registry
        )
        if approval_missing:
            coverage.setdefault("missing_points", [])
            coverage["missing_points"].extend(approval_missing)
            coverage["is_covered"] = False
        log_event("coverage_check", {"round": refine_round, "coverage": coverage})
        log_json("è¦†ç›–åº¦æ£€æŸ¥ç»“æœ", coverage)

        if coverage.get("is_covered", False):
            log_success("å½“å‰ç»“æ„å·²ç»è¢«åˆ¤å®šä¸ºâ€œå®Œå…¨è¦†ç›–â€ç”¨æˆ·éœ€æ±‚ã€‚")
            break

        missing_points = coverage.get("missing_points", []) or []
        if not missing_points:
            log_warn("è¦†ç›–åº¦æ£€æŸ¥è®¤ä¸ºä¸å®Œæ•´ï¼Œä½† missing_points ä¸ºç©ºï¼Œä¸å†å°è¯•ç»“æ„æ”¹è¿›ã€‚")
            break

        if refine_round == max_coverage_refine_rounds:
            log_warn("å·²è¾¾åˆ°æœ€å¤§ç»“æ„æ”¹è¿›è½®æ¬¡ï¼Œä»è®¤ä¸ºä¸å®Œå…¨è¦†ç›–ï¼Œä¿ç•™å½“å‰ç»“æ„ç»§ç»­åç»­é˜¶æ®µã€‚")
            break

        log_info("ğŸ”§ æ£€æµ‹åˆ°æœªè¦†ç›–çš„éœ€æ±‚ç‚¹ï¼Œå°†è°ƒç”¨ LLM å¯¹å·¥ä½œæµç»“æ„è¿›è¡Œå¢é‡æ”¹è¿›ï¼š")
        for mp in missing_points:
            log_debug(f" - {mp}")

        refined = refine_workflow_structure_with_llm(
            nl_requirement=nl_requirement,
            current_workflow=skeleton,
            missing_points=missing_points,
            model=OPENAI_MODEL,
        )

        refined_nodes = refined.get("nodes", [])
        refined_edges = refined.get("edges", [])
        refined["edges"] = ensure_edges_connectivity(refined_nodes, refined_edges)
        refined = ensure_registered_actions(
            refined, action_registry=action_registry, search_service=search_service
        )
        skeleton = refined

    skeleton = _ensure_loop_exports_with_llm(
        workflow=skeleton,
        action_registry=action_registry,
        nl_requirement=nl_requirement,
        model=OPENAI_MODEL,
    )

    return skeleton


__all__ = ["plan_workflow_structure_with_llm"]
