"""Lightweight run metrics collection and export helpers."""

from __future__ import annotations

import json
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Mapping

from velvetflow import logging_utils


def _coalesce_int(value: Any) -> int:
    try:
        return int(value)
    except Exception:  # noqa: BLE001
        return 0


@dataclass
class RunMetrics:
    total_nodes: int = 0
    failed_nodes: int = 0
    llm_calls: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0
    extra: dict[str, Any] = field(default_factory=dict)

    def record_node(self, *, success: bool) -> None:
        self.total_nodes += 1
        if not success:
            self.failed_nodes += 1

    def record_llm_usage(self, payload: Mapping[str, Any]) -> None:
        self.llm_calls += 1
        self.prompt_tokens += _coalesce_int(payload.get("prompt_tokens"))
        self.completion_tokens += _coalesce_int(payload.get("completion_tokens"))
        self.total_tokens += _coalesce_int(payload.get("total_tokens"))

    def to_dict(self, *, run_id: str | None = None, workflow_name: str | None = None) -> dict[str, Any]:
        payload = {
            "run_id": run_id,
            "workflow_name": workflow_name,
            "total_nodes": self.total_nodes,
            "failed_nodes": self.failed_nodes,
            "llm_calls": self.llm_calls,
            "prompt_tokens": self.prompt_tokens,
            "completion_tokens": self.completion_tokens,
            "total_tokens": self.total_tokens,
        }
        payload.update(self.extra)
        return payload

    def to_prometheus(self, *, run_id: str, workflow_name: str | None = None) -> str:
        labels = {"run_id": run_id}
        if workflow_name:
            labels["workflow"] = workflow_name
        label_text = ",".join(f'{k}="{v}"' for k, v in labels.items())
        lines = [
            "# HELP velvetflow_total_nodes Total nodes in the workflow run.",
            "# TYPE velvetflow_total_nodes gauge",
            f"velvetflow_total_nodes{{{label_text}}} {self.total_nodes}",
            "# HELP velvetflow_failed_nodes Number of nodes marked as failed.",
            "# TYPE velvetflow_failed_nodes gauge",
            f"velvetflow_failed_nodes{{{label_text}}} {self.failed_nodes}",
            "# HELP velvetflow_llm_calls LLM invocations observed during the run.",
            "# TYPE velvetflow_llm_calls counter",
            f"velvetflow_llm_calls{{{label_text}}} {self.llm_calls}",
            "# HELP velvetflow_prompt_tokens Prompt tokens consumed by LLMs.",
            "# TYPE velvetflow_prompt_tokens counter",
            f"velvetflow_prompt_tokens{{{label_text}}} {self.prompt_tokens}",
            "# HELP velvetflow_completion_tokens Completion tokens generated by LLMs.",
            "# TYPE velvetflow_completion_tokens counter",
            f"velvetflow_completion_tokens{{{label_text}}} {self.completion_tokens}",
            "# HELP velvetflow_total_tokens Total tokens consumed by LLMs.",
            "# TYPE velvetflow_total_tokens counter",
            f"velvetflow_total_tokens{{{label_text}}} {self.total_tokens}",
        ]
        return "\n".join(lines) + "\n"


class RunManager:
    """Context manager to manage per-run logging + metrics."""

    def __init__(
        self,
        *,
        workflow_name: str | None = None,
        run_id: str | None = None,
        log_dir: str | Path = "logs",
        metrics_format: str = "prometheus",
    ) -> None:
        self.workflow_name = workflow_name
        self.run_id = run_id or f"run-{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}-{uuid.uuid4().hex[:8]}"
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
        self.log_path = Path(logging_utils.LOG_FILE_PATH)
        self.run_log_path = self.log_dir / f"velvetflow_{ts}_{self.run_id}.log"
        self.metrics_path = self.log_dir / f"metrics_{ts}_{self.run_id}.txt"
        self.metrics_format = metrics_format
        self.metrics = RunMetrics()
        self._run_id_token = None

    def __enter__(self) -> "RunManager":
        self._run_id_token = logging_utils.configure_run_logging(
            self.run_id, self.run_log_path
        )
        logging_utils.set_llm_usage_recorder(self.metrics.record_llm_usage)
        return self

    def __exit__(self, exc_type, exc, tb) -> None:  # noqa: ANN001, D401
        logging_utils.set_llm_usage_recorder(None)
        if self._run_id_token:
            logging_utils._RUN_ID.reset(self._run_id_token)  # type: ignore[attr-defined]
        logging_utils.configure_run_logging(log_file=self.log_path)
        self.write_metrics()

    def write_metrics(self) -> Path:
        payload = self.metrics.to_dict(run_id=self.run_id, workflow_name=self.workflow_name)
        text = (
            self.metrics.to_prometheus(run_id=self.run_id, workflow_name=self.workflow_name)
            if self.metrics_format == "prometheus"
            else json.dumps(payload, ensure_ascii=False, indent=2)
        )
        with open(self.metrics_path, "w", encoding="utf-8") as fh:
            fh.write(text)
        logging_utils.log_event(
            "metrics_written",
            payload,
            workflow_run_id=self.run_id,
            level="INFO",
        )
        return self.metrics_path
